# ðŸ“Š GuÃ­a de Monitoreo con Grafana, Prometheus y ELK Stack

## CapÃ­tulo 10_1: SAGA + Redis + Observabilidad

---

## ðŸ“‹ Tabla de Contenidos

- [IntroducciÃ³n](#introducciÃ³n)
- [Â¿QuÃ© es Observabilidad?](#quÃ©-es-observabilidad)
- [Stack de Monitoreo](#stack-de-monitoreo)
- [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
- [Uso de Prometheus](#uso-de-prometheus)
- [Uso de Grafana](#uso-de-grafana)
- [Uso de ELK Stack](#uso-de-elk-stack)
- [MÃ©tricas Clave](#mÃ©tricas-clave)
- [Identificar Cuellos de Botella](#identificar-cuellos-de-botella)
- [Patrones de Error](#patrones-de-error)
- [Troubleshooting](#troubleshooting)

---

## ðŸŽ¯ IntroducciÃ³n

Este capÃ­tulo extiende el ejercicio de SAGA + Redis Cache agregando un **stack completo de observabilidad** para monitorear el comportamiento de los microservicios en tiempo real.

### Â¿QuÃ© aprenderÃ¡s?

- âœ… Configurar **Micrometer** para exponer mÃ©tricas desde Quarkus
- âœ… Usar **Prometheus** para recolectar y almacenar mÃ©tricas
- âœ… Crear **dashboards en Grafana** para visualizaciÃ³n
- âœ… Implementar **ELK Stack** (Elasticsearch + Logstash + Kibana) para logs centralizados
- âœ… Identificar **cuellos de botella** y patrones de rendimiento
- âœ… Detectar y analizar **patrones de error**
- âœ… Monitorear mÃ©tricas de **SAGA** (compensaciones, circuit breaker)
- âœ… Monitorear mÃ©tricas de **Redis Cache** (hit rate)

---

## ðŸ” Â¿QuÃ© es Observabilidad?

**Observabilidad** es la capacidad de entender el estado interno de un sistema basÃ¡ndose Ãºnicamente en sus salidas externas.

### Los 3 Pilares de la Observabilidad

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OBSERVABILIDAD                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1ï¸âƒ£  MÃ‰TRICAS (Metrics)                                â”‚
â”‚     â€¢ Â¿QuÃ© tan rÃ¡pido? Â¿CuÃ¡nto?                        â”‚
â”‚     â€¢ Prometheus + Grafana                             â”‚
â”‚     â€¢ Ejemplo: RPS, latencia, error rate               â”‚
â”‚                                                         â”‚
â”‚  2ï¸âƒ£  LOGS (Logs)                                        â”‚
â”‚     â€¢ Â¿QuÃ© pasÃ³? Â¿CuÃ¡ndo?                             â”‚
â”‚     â€¢ ELK Stack (Elasticsearch + Logstash + Kibana)    â”‚
â”‚     â€¢ Ejemplo: "Error en pago", "SAGA compensando"     â”‚
â”‚                                                         â”‚
â”‚  3ï¸âƒ£  TRAZAS (Traces)                                   â”‚
â”‚     â€¢ Â¿CÃ³mo fluye una request?                         â”‚
â”‚     â€¢ OpenTelemetry + Jaeger (no en este ejercicio)   â”‚
â”‚     â€¢ Ejemplo: Request atraviesa 3 servicios          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**AnalogÃ­a**: Observar un carro:
- **MÃ©tricas**: VelocÃ­metro, tacÃ³metro (datos numÃ©ricos)
- **Logs**: Computadora de diagnÃ³stico (eventos histÃ³ricos)
- **Trazas**: GPS mostrando la ruta completa (flujo de viaje)

---

## ðŸ› ï¸ Stack de Monitoreo

### Arquitectura del Sistema con Observabilidad

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   CLIENTE    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚ Order Serviceâ”‚
                    â”‚   (8080)     â”‚
                    â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”˜
                     â”‚             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Inventory    â”‚    â”‚  Payment     â”‚
         â”‚ Service      â”‚    â”‚  Service     â”‚
         â”‚  (8081)      â”‚    â”‚   (8082)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                  â”‚
                  â”‚    MÃ©tricas      â”‚
                  â–¼                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       PROMETHEUS (9090)         â”‚
         â”‚   Recolecta mÃ©tricas cada 15s   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        GRAFANA (3000)           â”‚
         â”‚   Visualiza mÃ©tricas            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â”‚                  â”‚
                  â”‚      Logs        â”‚
                  â–¼                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      LOGSTASH (5000)            â”‚
         â”‚   Procesa y parsea logs         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ELASTICSEARCH (9200)          â”‚
         â”‚   Almacena logs indexados       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       KIBANA (5601)             â”‚
         â”‚   Visualiza y busca logs        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes del Stack

| Componente | Puerto | FunciÃ³n |
|-----------|--------|---------|
| **Prometheus** | 9090 | RecolecciÃ³n de mÃ©tricas |
| **Grafana** | 3000 | VisualizaciÃ³n de mÃ©tricas |
| **Elasticsearch** | 9200 | Almacenamiento de logs |
| **Logstash** | 5000 | Procesamiento de logs |
| **Kibana** | 5601 | VisualizaciÃ³n de logs |
| **Filebeat** | - | Recolector de logs (opcional) |

---

## ðŸ“¦ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

- âœ… Docker Desktop corriendo
- âœ… 8 GB RAM disponibles (mÃ­nimo)
- âœ… Los 3 microservicios del CapÃ­tulo 10 funcionando

### Paso 1: Levantar el Stack de Monitoreo

```bash
# Desde la raÃ­z del proyecto CAPITULO 10_1
docker-compose -f docker-compose-monitoring.yml up -d
```

**Esto levanta:**
- PostgreSQL (ya existente)
- Redis (ya existente)
- Prometheus
- Grafana
- Elasticsearch
- Logstash
- Kibana
- Filebeat

**Verificar que estÃ©n corriendo:**
```bash
docker ps

# DeberÃ­as ver 8 contenedores activos
```

**Esperar a que estÃ©n listos (~2 minutos):**
```bash
# Verificar salud de cada contenedor
docker ps --format "table {{.Names}}\t{{.Status}}"
```

---

### Paso 2: Compilar los Microservicios con Micrometer

```bash
# Desde la raÃ­z del proyecto
mvn clean package -DskipTests

# Esto compila los 3 servicios con las nuevas dependencias de Micrometer
```

---

### Paso 3: Iniciar los 3 Microservicios

**Terminal 1 - Inventory Service:**
```bash
cd inventory-service
mvn quarkus:dev
```

**Terminal 2 - Payment Service:**
```bash
cd payment-service
mvn quarkus:dev
```

**Terminal 3 - Order Service:**
```bash
cd order-service
mvn quarkus:dev
```

---

### Paso 4: Verificar Endpoints de MÃ©tricas

```bash
# Verificar que cada servicio expone mÃ©tricas
curl http://localhost:8080/q/metrics  # Order Service
curl http://localhost:8081/q/metrics  # Inventory Service
curl http://localhost:8082/q/metrics  # Payment Service

# DeberÃ­as ver mÃ©tricas en formato Prometheus
```

**Salida esperada:**
```
# HELP jvm_memory_used_bytes The amount of used memory
# TYPE jvm_memory_used_bytes gauge
jvm_memory_used_bytes{area="heap",id="G1 Eden Space",} 2.45760E7
...
```

---

## ðŸ“Š Uso de Prometheus

### Acceder a Prometheus

1. Abrir navegador: http://localhost:9090
2. Ir a **Status > Targets**
3. Verificar que los 3 microservicios estÃ©n **UP**

```
Endpoint                              State    Last Scrape
order-service (localhost:8080)        UP       2s ago
inventory-service (localhost:8081)    UP       3s ago
payment-service (localhost:8082)      UP       1s ago
```

### Consultas BÃ¡sicas de Prometheus (PromQL)

#### 1. Requests por segundo (RPS)

```promql
# RPS total del Order Service
rate(http_server_requests_seconds_count{job="order-service"}[1m])

# RPS por endpoint
rate(http_server_requests_seconds_count{job="order-service", uri="/api/orders"}[1m])
```

#### 2. Latencia P95 (percentil 95)

```promql
# Latencia P95 del Order Service
histogram_quantile(0.95, 
  rate(http_server_requests_seconds_bucket{job="order-service"}[5m])
)
```

#### 3. Error Rate (tasa de errores)

```promql
# Porcentaje de errores 5xx
rate(http_server_requests_seconds_count{status=~"5..",job="order-service"}[5m]) 
/ 
rate(http_server_requests_seconds_count{job="order-service"}[5m])
```

#### 4. Uso de memoria JVM

```promql
# Memoria heap usada
jvm_memory_used_bytes{area="heap",job="order-service"}

# Memoria heap mÃ¡xima
jvm_memory_max_bytes{area="heap",job="order-service"}
```

#### 5. Redis Cache Hit Rate

```promql
# Cache Hit Rate (custom metric si la implementamos)
redis_cache_hit_total / (redis_cache_hit_total + redis_cache_miss_total)
```

### Ejercicio PrÃ¡ctico: Crear Alertas en Prometheus

Crear archivo `prometheus/alert_rules.yml`:

```yaml
groups:
  - name: microservices_alerts
    interval: 30s
    rules:
      # Alerta si error rate > 5%
      - alert: HighErrorRate
        expr: |
          rate(http_server_requests_seconds_count{status=~"5.."}[5m]) 
          / 
          rate(http_server_requests_seconds_count[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # Alerta si latencia P95 > 1 segundo
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_server_requests_seconds_bucket[5m])
          ) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "P95 latency is {{ $value }}s"
```

---

## ðŸ“ˆ Uso de Grafana

### Acceder a Grafana

1. Abrir navegador: http://localhost:3000
2. Login:
   - **Usuario**: `admin`
   - **ContraseÃ±a**: `admin`
3. (Opcional) Cambiar contraseÃ±a o skip

### Dashboard Pre-configurado

Ya viene un dashboard bÃ¡sico en `grafana/dashboards/microservices-dashboard.json`

Para verlo:
1. Ir a **Dashboards** (icono de 4 cuadrados)
2. Click en **Microservices Monitoring Dashboard**

### Crear Tu Propio Dashboard

#### Panel 1: Requests Per Second (RPS)

1. Click en **+ Create Dashboard**
2. Click en **Add new panel**
3. En la query, escribir:
   ```promql
   rate(http_server_requests_seconds_count{job="order-service"}[1m])
   ```
4. En **Legend**: `{{method}} {{uri}}`
5. Panel type: **Time series**
6. Guardar panel con tÃ­tulo: "RPS - Order Service"

#### Panel 2: Latencia P50, P95, P99

1. Agregar nuevo panel
2. Query A (P50):
   ```promql
   histogram_quantile(0.50, rate(http_server_requests_seconds_bucket{job="order-service"}[5m]))
   ```
3. Query B (P95):
   ```promql
   histogram_quantile(0.95, rate(http_server_requests_seconds_bucket{job="order-service"}[5m]))
   ```
4. Query C (P99):
   ```promql
   histogram_quantile(0.99, rate(http_server_requests_seconds_bucket{job="order-service"}[5m]))
   ```
5. Legend: `P50`, `P95`, `P99`
6. Unit: `seconds (s)`

#### Panel 3: Error Rate Gauge

1. Agregar panel tipo **Gauge**
2. Query:
   ```promql
   rate(http_server_requests_seconds_count{status=~"5.."}[5m]) 
   / 
   rate(http_server_requests_seconds_count[5m])
   ```
3. Unit: **Percent (0.0-1.0)**
4. Thresholds:
   - Verde: 0 - 0.01 (1%)
   - Amarillo: 0.01 - 0.05 (5%)
   - Rojo: > 0.05

#### Panel 4: Circuit Breaker State

1. Agregar panel tipo **Stat**
2. Query:
   ```promql
   resilience4j_circuitbreaker_state{name="order-saga"}
   ```
3. Value mappings:
   - 0 = CLOSED (verde)
   - 1 = OPEN (rojo)
   - 2 = HALF_OPEN (amarillo)

#### Panel 5: JVM Memory Usage

1. Agregar panel
2. Query A (Used):
   ```promql
   jvm_memory_used_bytes{area="heap"}
   ```
3. Query B (Max):
   ```promql
   jvm_memory_max_bytes{area="heap"}
   ```
4. Legend: `{{job}} - Used` y `{{job}} - Max`
5. Unit: **bytes (IEC)**

### Guardar Dashboard

1. Click en **Save dashboard** (icono de disquete)
2. Nombre: "Microservices SAGA & Redis Monitoring"
3. Click en **Save**

---

## ðŸ“‹ Uso de ELK Stack

### Acceder a Kibana

1. Abrir navegador: http://localhost:5601
2. Esperar a que Kibana cargue (puede tomar 1-2 minutos)

### Configurar Index Pattern

1. Ir a **Management > Stack Management**
2. Click en **Index Patterns**
3. Click en **Create index pattern**
4. Index pattern name: `quarkus-logs-*`
5. Time field: `@timestamp`
6. Click en **Create index pattern**

### Buscar Logs

1. Ir a **Discover** (icono de brÃºjula)
2. Seleccionar index pattern: `quarkus-logs-*`
3. Ver logs en tiempo real

### Filtros Ãštiles

#### Ver solo logs de SAGA

```
tags: saga
```

#### Ver solo errores

```
level: ERROR
```

#### Ver compensaciones de SAGA

```
tags: saga-compensation
```

#### Ver logs de un servicio especÃ­fico

```
service_name: "order-service"
```

### Crear Visualizaciones

#### VisualizaciÃ³n 1: Logs por Nivel

1. Ir a **Visualize Library**
2. Click en **Create visualization**
3. Tipo: **Pie chart**
4. Data source: `quarkus-logs-*`
5. Metrics:
   - Aggregation: **Count**
6. Buckets:
   - Aggregation: **Terms**
   - Field: `level.keyword`
7. Guardar como: "Logs by Level"

#### VisualizaciÃ³n 2: Errores en el Tiempo

1. Crear visualizaciÃ³n tipo **Area**
2. Metrics: **Count**
3. Buckets:
   - X-axis: **Date Histogram**
   - Field: `@timestamp`
4. Filters: `level: ERROR`
5. Guardar como: "Errors Over Time"

#### VisualizaciÃ³n 3: Top Servicios con Errores

1. Tipo: **Horizontal Bar**
2. Metrics: **Count**
3. Buckets:
   - Y-axis: **Terms**
   - Field: `service_name.keyword`
4. Filters: `level: ERROR`
5. Guardar como: "Services with Most Errors"

### Crear Dashboard en Kibana

1. Ir a **Dashboard**
2. Click en **Create dashboard**
3. Agregar las 3 visualizaciones creadas
4. Guardar como: "Microservices Logs Dashboard"

---

## ðŸ” MÃ©tricas Clave

### 1. MÃ©tricas de AplicaciÃ³n (RED Method)

| MÃ©trica | PromQL | DescripciÃ³n |
|---------|--------|-------------|
| **Rate** (RPS) | `rate(http_server_requests_seconds_count[1m])` | Requests por segundo |
| **Errors** | `rate(http_server_requests_seconds_count{status=~"5.."}[5m])` | Errores por segundo |
| **Duration** (Latencia) | `histogram_quantile(0.95, rate(http_server_requests_seconds_bucket[5m]))` | Tiempo de respuesta P95 |

### 2. MÃ©tricas de SAGA

| MÃ©trica | DÃ³nde verla | QuÃ© indica |
|---------|-------------|------------|
| **SAGA Success Rate** | Logs con tag `saga` sin `saga-compensation` | % de SAGAs exitosas |
| **SAGA Compensation Rate** | Logs con tag `saga-compensation` | % de compensaciones ejecutadas |
| **SAGA Duration** | Latencia del endpoint `/api/orders` | Tiempo total de la transacciÃ³n distribuida |

### 3. MÃ©tricas de Redis Cache

| MÃ©trica | DÃ³nde verla | QuÃ© indica |
|---------|-------------|------------|
| **Cache Hit Rate** | Logs con "Cache HIT" vs "Cache MISS" | Efectividad del cache |
| **Cache Latency** | Latencia de llamadas con cache | Beneficio de usar cache |

### 4. MÃ©tricas de Circuit Breaker

| MÃ©trica | PromQL | QuÃ© indica |
|---------|--------|------------|
| **Circuit State** | `resilience4j_circuitbreaker_state` | Estado actual (CLOSED/OPEN/HALF_OPEN) |
| **Failure Rate** | `resilience4j_circuitbreaker_failure_rate` | % de fallos que causaron la apertura |

### 5. MÃ©tricas de JVM

| MÃ©trica | PromQL | QuÃ© indica |
|---------|--------|------------|
| **Heap Memory** | `jvm_memory_used_bytes{area="heap"}` | Memoria usada |
| **GC Pause** | `jvm_gc_pause_seconds` | Tiempo en Garbage Collection |
| **Threads** | `jvm_threads_live` | NÃºmero de threads activos |

---

## ðŸŒ Identificar Cuellos de Botella

### MetodologÃ­a: AnÃ¡lisis de Latencia

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REQUEST COMPLETA: 500ms                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Order Service recibe request         â”‚  10ms       â”‚
â”‚ 2. Buscar producto en cache (MISS)      â”‚  50ms  âš ï¸   â”‚
â”‚ 3. SAGA Step 1: Reserve Inventory       â”‚  150ms âš ï¸   â”‚
â”‚ 4. SAGA Step 2: Process Payment         â”‚  200ms ðŸ”´   â”‚
â”‚ 5. SAGA Step 3: Confirm Inventory       â”‚  80ms       â”‚
â”‚ 6. Save order to DB                     â”‚  10ms       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ” AnÃ¡lisis:
- ðŸ”´ Payment Service es el cuello de botella (200ms = 40% del total)
- âš ï¸ Inventory Service tambiÃ©n es lento (150ms)
- âš ï¸ Cache MISS agrega 50ms extra
```

### Paso 1: Identificar el Servicio MÃ¡s Lento

**En Grafana**, crear query para comparar latencias:

```promql
# Latencia P95 de cada servicio
histogram_quantile(0.95, 
  rate(http_server_requests_seconds_bucket[5m])
)
```

**Agrupar por servicio**:
```promql
histogram_quantile(0.95, 
  sum by (job) (rate(http_server_requests_seconds_bucket[5m]))
)
```

### Paso 2: Identificar el Endpoint MÃ¡s Lento

```promql
# Latencia P95 por endpoint
histogram_quantile(0.95, 
  sum by (uri) (rate(http_server_requests_seconds_bucket{job="order-service"}[5m]))
)
```

### Paso 3: Correlacionar con Logs en Kibana

1. Ir a Kibana > Discover
2. Filtrar por el servicio lento:
   ```
   service_name: "payment-service" AND level: DEBUG
   ```
3. Buscar patrones:
   - Â¿Hay muchas consultas a BD?
   - Â¿Hay timeouts?
   - Â¿Hay reintentos?

### Ejemplo de AnÃ¡lisis Completo

**Escenario**: Payment Service es lento (P95 = 800ms)

**Paso 1 - MÃ©tricas en Grafana**:
```promql
# Payment Service latencia por endpoint
histogram_quantile(0.95, 
  sum by (uri) (rate(http_server_requests_seconds_bucket{job="payment-service"}[5m]))
)

# Resultado: /api/payments/process = 800ms
```

**Paso 2 - Logs en Kibana**:
```
service_name: "payment-service" AND uri: "/api/payments/process"

# Encontramos:
2024-10-23 20:30:45 DEBUG PaymentService - Validando tarjeta... (50ms)
2024-10-23 20:30:46 DEBUG PaymentService - Consultando fraude externo... (700ms) ðŸ”´
2024-10-23 20:30:46 DEBUG PaymentService - Guardando pago... (50ms)
```

**ConclusiÃ³n**: El servicio externo de fraude es el cuello de botella.

**Soluciones**:
1. Cachear resultados de validaciÃ³n de fraude
2. Hacer la consulta asÃ­ncrona
3. Implementar circuit breaker con fallback

---

## âŒ Patrones de Error

### 1. Error Intermitente (Transient Errors)

**SÃ­ntomas en Grafana**:
- Error rate con picos pero retorna a 0%
- GrÃ¡fica de "dientes de sierra"

**Ejemplo en Kibana**:
```
level: ERROR AND message: "Connection timeout"
```

**Causa comÃºn**: Servicio externo inestable

**SoluciÃ³n**: Implementar retry con backoff exponencial

---

### 2. Error en Cascada

**SÃ­ntomas**:
- Un servicio falla â†’ Todos los servicios muestran errores
- Error rate aumenta en todos los servicios al mismo tiempo

**Ejemplo**:
```
T0: Payment Service falla (error rate 100%)
T1: Order Service falla porque Payment falla (error rate 100%)
T2: Clientes reciben errores (UX degradada)
```

**SoluciÃ³n**: Circuit Breaker (ya implementado)

---

### 3. Memory Leak

**SÃ­ntomas en Grafana**:
- Memoria heap crece constantemente
- GC pause time aumenta
- Eventualmente: OutOfMemoryError

```promql
# Ver tendencia de memoria
jvm_memory_used_bytes{area="heap",job="order-service"}
```

**En Kibana**:
```
level: ERROR AND message: "OutOfMemoryError"
```

**SoluciÃ³n**: Heap dump analysis con VisualVM

---

### 4. SAGA Compensation Storm

**SÃ­ntomas**:
- Muchas compensaciones al mismo tiempo
- Logs llenos de "ðŸ”„ compensando..."

**En Kibana**:
```
tags: saga-compensation

# Contar compensaciones por minuto
```

**Causa**: Servicio downstream fallando consistentemente

**SoluciÃ³n**: Implementar fallback + circuit breaker

---

## ðŸ› ï¸ Troubleshooting

### Problema 1: Prometheus no scrapea mÃ©tricas

**SÃ­ntoma**:
```
Status > Targets â†’ order-service: DOWN
```

**DiagnÃ³stico**:
```bash
# Verificar que el servicio expone mÃ©tricas
curl http://localhost:8080/q/metrics

# Si falla, verificar logs del servicio
```

**Soluciones**:
1. Verificar que Micrometer estÃ© habilitado en `application.properties`
2. Verificar que el servicio estÃ© corriendo
3. En Docker, usar `host.docker.internal` en lugar de `localhost`

---

### Problema 2: Grafana no muestra datos

**SÃ­ntoma**: Panels vacÃ­os con "No data"

**DiagnÃ³stico**:
1. Verificar datasource: **Configuration > Data sources > Prometheus**
2. Click en **Test**: Debe decir "Data source is working"
3. Verificar que hay mÃ©tricas en Prometheus: http://localhost:9090

**SoluciÃ³n**:
```bash
# Reiniciar Grafana
docker restart grafana
```

---

### Problema 3: Kibana no muestra logs

**SÃ­ntoma**: Index pattern sin datos

**DiagnÃ³stico**:
```bash
# Verificar que Elasticsearch estÃ¡ corriendo
curl http://localhost:9200/_cluster/health

# Verificar que Logstash estÃ¡ procesando
docker logs logstash

# Verificar Ã­ndices creados
curl http://localhost:9200/_cat/indices
```

**SoluciÃ³n**:
1. Verificar que los microservicios estÃ¡n enviando logs a Logstash
2. Verificar configuraciÃ³n de `logstash.conf`
3. Reiniciar stack ELK:
   ```bash
   docker restart elasticsearch logstash kibana
   ```

---

### Problema 4: Elasticsearch se queda sin memoria

**SÃ­ntoma**:
```bash
docker logs elasticsearch
# ERROR: OutOfMemoryError
```

**SoluciÃ³n**:
Aumentar heap size en `docker-compose-monitoring.yml`:
```yaml
elasticsearch:
  environment:
    - "ES_JAVA_OPTS=-Xms1g -Xmx1g"  # Aumentar a 1GB
```

---

## ðŸŽ¯ Ejercicios PrÃ¡cticos

### Ejercicio 1: Detectar Cache Miss Ratio Alto

**Objetivo**: Usar Grafana para ver el cache hit rate de Redis

**Pasos**:
1. Generar trÃ¡fico:
   ```bash
   # Crear 100 Ã³rdenes con productos diferentes
   for i in {1..100}; do
     curl -X POST http://localhost:8080/api/orders \
       -H "Content-Type: application/json" \
       -d "{...}"
   done
   ```

2. En Grafana, buscar en logs:
   - "Cache HIT"
   - "Cache MISS"

3. Calcular ratio: `HIT / (HIT + MISS)`

**Pregunta**: Â¿CÃ³mo mejorarÃ­as un cache hit rate de 30%?

---

### Ejercicio 2: Simular Fallo en Payment Service

**Objetivo**: Ver cÃ³mo se comportan las mÃ©tricas y logs durante un fallo

**Pasos**:
1. Detener Payment Service:
   ```bash
   # En la terminal donde corre payment-service
   Ctrl + C
   ```

2. Crear Ã³rdenes:
   ```bash
   curl -X POST http://localhost:8080/api/orders ...
   ```

3. En Grafana, observar:
   - Error rate sube a 100%
   - Circuit breaker se abre

4. En Kibana, buscar:
   ```
   tags: saga-compensation
   ```

5. Reiniciar Payment Service y observar recuperaciÃ³n

---

### Ejercicio 3: Identificar Cuello de Botella

**Objetivo**: Usar mÃ©tricas para encontrar el servicio mÃ¡s lento

**Pasos**:
1. Generar trÃ¡fico sostenido
2. En Grafana, comparar latencias:
   ```promql
   histogram_quantile(0.95, 
     sum by (job) (rate(http_server_requests_seconds_bucket[5m]))
   )
   ```
3. Identificar el servicio con mayor latencia
4. Ir a Kibana y buscar logs de ese servicio
5. Proponer soluciÃ³n

---

## ðŸ“š Recursos Adicionales

### DocumentaciÃ³n

- [Prometheus Query Language (PromQL)](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)
- [Kibana Query Language (KQL)](https://www.elastic.co/guide/en/kibana/current/kuery-query.html)
- [Quarkus Micrometer Guide](https://quarkus.io/guides/micrometer)

### Dashboards PÃºblicos

- [JVM Dashboard for Grafana](https://grafana.com/grafana/dashboards/4701)
- [Quarkus Dashboard](https://grafana.com/grafana/dashboards/14370)

---

## ðŸŽ“ ConclusiÃ³n

Has aprendido a:

âœ… Configurar un **stack completo de observabilidad** para microservicios  
âœ… Usar **Prometheus** para recolectar mÃ©tricas  
âœ… Crear **dashboards en Grafana** para visualizaciÃ³n en tiempo real  
âœ… Implementar **ELK Stack** para logs centralizados  
âœ… **Identificar cuellos de botella** usando mÃ©tricas de latencia  
âœ… **Detectar patrones de error** correlacionando mÃ©tricas y logs  
âœ… Monitorear comportamiento de **SAGA** y **Redis Cache**  

### ðŸš€ PrÃ³ximos Pasos

1. Implementar **distributed tracing** con OpenTelemetry + Jaeger
2. Agregar **alerting** con Prometheus Alertmanager
3. Crear **SLOs** (Service Level Objectives) para tus servicios
4. Implementar **synthetic monitoring** con pruebas automatizadas

---

**Â¡Felicitaciones por completar el CapÃ­tulo 10_1! ðŸŽ‰**
